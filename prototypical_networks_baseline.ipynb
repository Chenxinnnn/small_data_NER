{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55bVR1nL8DJP",
    "outputId": "110b89ef-443f-4f36-b32e-f09edab2672d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval -q\n",
    "!pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNrZnrqn8DJQ",
    "outputId": "eed8eea4-931e-4d37-af83-94b9846be05b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkehKS_E8DJQ",
    "outputId": "42732e86-f6ba-4504-e6f2-49ebaaf08311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: train=2 dev=200 test=851\n",
      "Sample: ['He', 'had', 'a', 'medical', 'history', 'of', 'diabetes', 'mellitus', ',', 'hypertension', 'and', 'he'] \n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'B-ety', 'I-ety', 'O', 'B-ety', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# ===== Prototypical Networks: Token-level few-shot NER (BioBERT encoder) =====\n",
    "import os, random, numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModel,\n",
    "                          DataCollatorForTokenClassification, TrainingArguments, Trainer)\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "# ---- paths ----\n",
    "BASE = Path(\"/content/drive/MyDrive/small_data_NER\")\n",
    "DATA_DIR = BASE / \"conll/fewshot_k10_seed42_mention\"   # <-- change to fewshot_k1_seed42 / k10 / k20 if needed\n",
    "OUT_DIR  = BASE / \"results\"/\"proto_net_baseline_k5_full\"\n",
    "\n",
    "# ---- read CoNLL ----\n",
    "def read_conll(path):\n",
    "    sents, tokens, labels = [], [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append({\"tokens\":tokens, \"ner_tags\":labels})\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                tok, lab = parts[0], parts[-1]\n",
    "                tokens.append(tok); labels.append(lab)\n",
    "    if tokens: sents.append({\"tokens\":tokens, \"ner_tags\":labels})\n",
    "    return sents\n",
    "\n",
    "train = read_conll(DATA_DIR/\"train.conll\")\n",
    "dev   = read_conll(DATA_DIR/\"dev.conll\")\n",
    "test  = read_conll(DATA_DIR/\"test.conll\")\n",
    "\n",
    "print(f\"Loaded: train={len(train)} dev={len(dev)} test={len(test)}\")\n",
    "print(\"Sample:\", train[0][\"tokens\"][:12], \"\\n\", train[0][\"ner_tags\"][:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "referenced_widgets": [
      "8196e3c88bfb41a8810f7914a1504d06",
      "b830069a34624b1d994de6e1666a4ff4",
      "0c84f4ead63a43ab9c1ca1b6314622a7",
      "5b2b074236b14ce49116482e9b75dc94",
      "72fb273f91c545e2b628d1919a3ad1fa",
      "d4944625e27d4c8c91bca29dc2a2da0d",
      "fd7c3c5abf5b4f3b93513cd19d3f0075",
      "ddd349d65d3745869fc210a27b4e9a3e",
      "29d63b3e670b431eb5e57ed00aee4bed",
      "5120d8fdd7b547b69fc3ed8b30268042",
      "34cdaeb1ed5742ec92a62ad3a973a71c",
      "3f05760bc4744f51a90812bdb992504f",
      "4b7be12d6aba443da115388aa16172d9",
      "1b0e91e336e543acb7fdd42bc3f80e7b",
      "7793dd2c186b470e9f7116e8f4bd4f55",
      "5bf2b5e281f34d47a3bea88d40e88daf",
      "27dc8f4943524247a8a749d4c76e2e85",
      "62a0f8e1156a4623a4b163ef16534940",
      "2f96c1b334004443869788776dd2ee3f",
      "8640a5e88e2e4a51b961405b81265dbe",
      "68fb855721334de2a68a4d4ad4187532",
      "6e489759d09a47908a9307ca211b4ffc",
      "8bd686454d3846a3b5d464450036f310",
      "ee7cc349ad3c44e4bed48afdf6ef2a22",
      "2e5d0c9b79bf459bad590f66dc2a8c23",
      "57735478c6d74796bfc3cf6a69df0c38",
      "845c548dd39d43449359474564d225a1",
      "c0917fddc7b04229bd640575bf209d76",
      "904c81112fcb4d4e9fe337eb4b960218",
      "040a17cb57f548c1bf7a48626d61b223",
      "cbcb91cacded4f53a2cbbb476db52ee2",
      "826b7792fedf4cd88e58e203fb16763e",
      "7dc145c5b489436fbf1fc63038bacba2",
      "1a1afdb74f904e9e9fdc8268003de179",
      "404cb0545bb841388ad30b944751a1a8",
      "b5d7aba190d843aea00f25164dffc566",
      "3558c84c231445d18175b973779c70e0",
      "fa8d056a14f14d2f8f0df64e253c603f",
      "2c89bf5cf72f4cea8e0c5861be8ab0d1",
      "3e5701257e2c469cb1c33ab4cd2f3b73",
      "84fac1dbb9d64eb9acf6d3fbbc533dfa",
      "9fc50b2de618485d92e5097299d40b77",
      "110bb4435e304cf4ac22df840a7fead5",
      "a027af2d09c744d0b483f2f6f7003103",
      "cac5a6c60b154351b4b8b47bb629e6c5",
      "82db19563ff6429894b922db524f02b3",
      "7cb155c0627444c3891e10780e05d7de",
      "9c263b842325407496dff73602ab4647",
      "f6528eaaf5e247cdba8cd02455436f66",
      "16c79b0078a24a8f8d6cc59d70ff9df4",
      "8f97d95ed2b34dc9954e308769a0c1eb",
      "d9ec0f3ab8d444f2b91ffc6336ec566b",
      "a6e28b6325b448a3b4ddd8c4222eec1d",
      "8fce768294424fe2b9f2c4729e16973d",
      "9e6980b361fb446781c22a43599ef0ca"
     ]
    },
    "id": "lLBbc65T8DJQ",
    "outputId": "0fc90a1b-bb91-4231-e943-7cf4de07a152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-ety', 'I-ety']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8196e3c88bfb41a8810f7914a1504d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f05760bc4744f51a90812bdb992504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd686454d3846a3b5d464450036f310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1afdb74f904e9e9fdc8268003de179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac5a6c60b154351b4b8b47bb629e6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- build label list (BIO) ----\n",
    "all_labels = sorted({l for ex in (train+dev+test) for l in ex[\"ner_tags\"]})\n",
    "if \"O\" in all_labels:\n",
    "    all_labels.remove(\"O\"); all_labels = [\"O\"] + all_labels\n",
    "label2id = {l:i for i,l in enumerate(all_labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "num_labels = len(all_labels)\n",
    "print(\"Labels:\", all_labels)\n",
    "\n",
    "# ---- HF datasets ----\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train),\n",
    "    \"validation\": Dataset.from_list(dev),\n",
    "    \"test\": Dataset.from_list(test),\n",
    "})\n",
    "\n",
    "# ---- tokenizer & alignment ----\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"  # encoder backbone\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_align(batch):\n",
    "    tokenized = tokenizer(batch[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "    for i, lbls in enumerate(batch[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "        prev_word = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned.append(-100)\n",
    "            else:\n",
    "                # Only label the first wordpiece; rest -> -100\n",
    "                if wid != prev_word:\n",
    "                    aligned.append(label2id.get(lbls[wid], label2id[\"O\"]))\n",
    "                else:\n",
    "                    aligned.append(-100)\n",
    "                prev_word = wid\n",
    "        labels.append(aligned)\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized = ds.map(tokenize_align, batched=True, remove_columns=[\"tokens\",\"ner_tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "881Vu5Uv8DJR",
    "outputId": "d390e553-252b-41ac-f7d3-9a88fc2e8ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-ety', 'I-ety']\n",
      "✓ label vocab OK\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", all_labels)\n",
    "assert all_labels[0] == \"O\"\n",
    "assert set(all_labels) >= {\"B-ety\",\"I-ety\"}  # 若只有一种实体类型\n",
    "print(\"✓ label vocab OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tQUfBYA8DJR",
    "outputId": "75211c02-a846-49e8-db8f-830f790b2e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eff labels (train): 58\n",
      "eff labels (dev)  : 3545\n",
      "eff labels (test) : 16702\n"
     ]
    }
   ],
   "source": [
    "def effective_labels_count(tokenized_split):\n",
    "    return sum(int(x!=-100) for ex in tokenized_split[\"labels\"] for x in ex)\n",
    "\n",
    "print(\"eff labels (train):\", effective_labels_count(tokenized[\"train\"]))\n",
    "print(\"eff labels (dev)  :\", effective_labels_count(tokenized[\"validation\"]))\n",
    "print(\"eff labels (test) :\", effective_labels_count(tokenized[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9sRcaUv38DJR",
    "outputId": "9908d0a8-bafd-400a-a4d6-e5b091906307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique label ids (train): [0, 1, 2] ['O', 'B-ety', 'I-ety']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unique_effective_ids(tokenized_split):\n",
    "    ids = []\n",
    "    for ex in tokenized_split[\"labels\"]:\n",
    "        ids.extend([i for i in ex if i!=-100])\n",
    "    return sorted(set(ids))\n",
    "\n",
    "uids_train = unique_effective_ids(tokenized[\"train\"])\n",
    "print(\"unique label ids (train):\", uids_train, [id2label[i] for i in uids_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdwiA4iu8DJR",
    "outputId": "88120855-4f88-4f1c-8312-f30a373b3840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0.11428571492433548, 'B-ety': 0.6000000238418579, 'I-ety': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# ---- class weights (reduce 'O') ----\n",
    "from collections import Counter\n",
    "import torch, numpy as np\n",
    "\n",
    "cnt = Counter(l for ex in train for l in ex[\"ner_tags\"])\n",
    "weights = np.array([cnt.get(l,1) for l in all_labels], dtype=float)\n",
    "weights = 1.0 / weights\n",
    "weights /= weights.max()\n",
    "weights[label2id['O']] *= 0.8\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "print({l: float(class_weights[label2id[l]]) for l in all_labels})\n",
    "\n",
    "# ---- metrics ----\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    pred_tags, true_tags = [], []\n",
    "    for p, l in zip(preds, labels):\n",
    "        pt, lt = [], []\n",
    "        for pi, li in zip(p, l):\n",
    "            if li == -100:  # skip subword positions\n",
    "                continue\n",
    "            pt.append(id2label[int(pi)])\n",
    "            lt.append(id2label[int(li)])\n",
    "        pred_tags.append(pt); true_tags.append(lt)\n",
    "    f1 = f1_score(true_tags, pred_tags)\n",
    "    return {\"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Prototypical Token Classifier (Frozen encoder + trainable projection) ----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class ProtoTokenClassifierFrozen(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, id2label=None, label2id=None, proj_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        # freeze encoder parameters\n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_labels = num_labels\n",
    "        self.id2label = id2label or {}\n",
    "        self.label2id = label2id or {}\n",
    "        # a small projection head; default proj_dim=hidden\n",
    "        dim = proj_dim or hidden\n",
    "        self.proj = nn.Sequential(nn.Linear(hidden, dim), nn.Tanh(), nn.LayerNorm(dim))\n",
    "        # static prototypes for eval/inference (C, D)\n",
    "        self.register_buffer(\"static_prototypes\", None, persistent=False)\n",
    "\n",
    "    def set_static_prototypes(self, protos):\n",
    "        self.static_prototypes = protos\n",
    "\n",
    "    def _features(self, **inputs):\n",
    "        out = self.encoder(**inputs)\n",
    "        x = self.dropout(out.last_hidden_state)\n",
    "        x = self.proj(x)  # [B,T,D]\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _pairwise_sq_dist(x, y):\n",
    "        x2 = (x**2).sum(-1, keepdim=True)\n",
    "        y2 = (y**2).sum(-1).unsqueeze(0)\n",
    "        xy = x @ y.t()\n",
    "        return x2 + y2 - 2*xy\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, prototypes=None):\n",
    "        feats = self._features(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        B, T, D = feats.shape\n",
    "        device = feats.device\n",
    "        protos = prototypes\n",
    "        if protos is None:\n",
    "            if labels is not None:\n",
    "                feats_flat = feats.reshape(-1, D)\n",
    "                labels_flat = labels.view(-1)\n",
    "                protos_list = []\n",
    "                for c in range(self.num_labels):\n",
    "                    mask_c = (labels_flat == c)\n",
    "                    if mask_c.any():\n",
    "                        pc = feats_flat[mask_c].mean(dim=0)\n",
    "                    else:\n",
    "                        if self.static_prototypes is not None:\n",
    "                            pc = self.static_prototypes[c]\n",
    "                        else:\n",
    "                            pc = torch.zeros(D, device=device)\n",
    "                    protos_list.append(pc)\n",
    "                protos = torch.stack(protos_list, dim=0)  # [C,D]\n",
    "            elif self.static_prototypes is not None:\n",
    "                protos = self.static_prototypes\n",
    "        if protos is None:\n",
    "            logits = torch.zeros(B, T, self.num_labels, device=device)\n",
    "        else:\n",
    "            x = feats.reshape(-1, D)\n",
    "            dists = self._pairwise_sq_dist(x, protos)\n",
    "            logits = (-dists).reshape(B, T, self.num_labels)\n",
    "        return TokenClassifierOutput(logits=logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_prototypes_from_dataset(self, dataloader, device, num_labels):\n",
    "        sums = None\n",
    "        counts = torch.zeros(num_labels, dtype=torch.long, device=device)\n",
    "        for batch in dataloader:\n",
    "            for k in list(batch.keys()):\n",
    "                if isinstance(batch[k], torch.Tensor):\n",
    "                    batch[k] = batch[k].to(device)\n",
    "            labels = batch.get('labels', None)\n",
    "            feats = self._features(input_ids=batch['input_ids'], attention_mask=batch.get('attention_mask'), token_type_ids=batch.get('token_type_ids'))\n",
    "            B, T, D = feats.shape\n",
    "            if sums is None:\n",
    "                sums = torch.zeros(num_labels, D, device=device)\n",
    "            feats_flat = feats.reshape(-1, D)\n",
    "            if labels is None:\n",
    "                continue\n",
    "            labels_flat = labels.view(-1)\n",
    "            for c in range(num_labels):\n",
    "                mask_c = (labels_flat == c)\n",
    "                if mask_c.any():\n",
    "                    sums[c] += feats_flat[mask_c].sum(dim=0)\n",
    "                    counts[c] += mask_c.sum()\n",
    "        protos = torch.where(counts.view(-1,1) > 0, sums / counts.clamp(min=1).view(-1,1), torch.zeros_like(sums))\n",
    "        return protos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VuQu8bY_8DJS"
   },
   "outputs": [],
   "source": [
    "# ---- training args ----\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=str(OUT_DIR / \"logs\"),\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    seed=42,\n",
    "    report_to=None\n",
    ")\n",
    "collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MbXtqkO58DJS"
   },
   "outputs": [],
   "source": [
    "# ---- ProtoTrainer with weighted CE over -distance logits ----\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProtoTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self._static_protos_ready = False\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs, labels=labels)  # in-batch prototypes\n",
    "        logits = outputs.logits\n",
    "        w = None\n",
    "        if self.class_weights is not None:\n",
    "            w = self.class_weights.to(logits.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=w, ignore_index=-100, label_smoothing=0.1)\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def _ensure_static_prototypes(self):\n",
    "        if self._static_protos_ready:\n",
    "            return\n",
    "        # build dataloader over train set\n",
    "        dl = self.get_eval_dataloader(self.train_dataset)\n",
    "        # Get device from model parameters\n",
    "        device = next(self.model.parameters()).device\n",
    "        protos = self.model.compute_prototypes_from_dataset(dl, device, num_labels)\n",
    "        self.model.set_static_prototypes(protos)\n",
    "        self._static_protos_ready = True\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self._ensure_static_prototypes()\n",
    "        return super().evaluate(*args, **kwargs)\n",
    "\n",
    "    def predict(self, *args, **kwargs):\n",
    "        self._ensure_static_prototypes()\n",
    "        return super().predict(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- model + trainer ----\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "model = ProtoTokenClassifierFrozen(MODEL_NAME, num_labels=num_labels, id2label=id2label, label2id=label2id, proj_dim=None)\n",
    "trainer = ProtoTrainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=tokenized[\"train\"], eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer, data_collator=collator, compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "mHEz9GTq8DJS",
    "outputId": "a8dfbcb0-755f-4041-d615-a88cbfc49a03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred label dist on DEV: Counter({'O': 3095, 'B-ety': 228, 'I-ety': 222})\n"
     ]
    }
   ],
   "source": [
    "# dev label distribution diagnostics\n",
    "out = trainer.predict(tokenized[\"validation\"])\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "pred_ids = np.argmax(out.predictions, axis=-1)\n",
    "true_ids = out.label_ids\n",
    "\n",
    "pred_tags = []\n",
    "for p, l in zip(pred_ids, true_ids):\n",
    "    pred_tags += [id2label[int(pi)] for pi, li in zip(p, l) if li != -100]\n",
    "print(\"Pred label dist on DEV:\", Counter(pred_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAWWYQP98DJS",
    "outputId": "1e5c5750-a8b3-4f6d-9c66-4dc69e517a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold label dist on DEV: Counter({'O': 3285, 'B-ety': 134, 'I-ety': 126})\n"
     ]
    }
   ],
   "source": [
    "true_tags = []\n",
    "for l in true_ids:\n",
    "    true_tags += [id2label[int(li)] for li in l if li != -100]\n",
    "print(\"Gold label dist on DEV:\", Counter(true_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ravgd4SF8DJT",
    "outputId": "5872adcc-cad0-4d91-91c8-4ae66725f3df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n",
      "cuda? False\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch; print(\"cuda?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "xzOELwgd8DJT",
    "outputId": "1e946479-7a5b-4aa6-f218-c20b9f9c7376"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION F1: 0.5097\n",
      "TEST F1: 0.559\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ety       0.41      0.88      0.56       516\n",
      "\n",
      "   micro avg       0.41      0.88      0.56       516\n",
      "   macro avg       0.41      0.88      0.56       516\n",
      "weighted avg       0.41      0.88      0.56       516\n",
      "\n",
      "\n",
      "Saved metrics to /content/drive/MyDrive/small_data_NER/results/proto_net_k5_full/metrics.json\n"
     ]
    }
   ],
   "source": [
    "# ---- evaluate (dev + test) ----\n",
    "import time\n",
    "def eval_split(name):\n",
    "    out = trainer.evaluate(tokenized[name])\n",
    "    print(f\"{name.upper()} F1:\", round(out[\"eval_f1\"], 4))\n",
    "    return out[\"eval_f1\"]\n",
    "\n",
    "f1_dev  = eval_split(\"validation\")\n",
    "f1_test = eval_split(\"test\")\n",
    "\n",
    "# ---- save predictions + detailed report on test ----\n",
    "t0 = time.time()\n",
    "pred = trainer.predict(tokenized[\"test\"])\n",
    "inference_time = time.time() - t0\n",
    "pred_logits = pred.predictions\n",
    "pred_ids = pred_logits.argmax(-1)\n",
    "pred_tags, true_tags = [], []\n",
    "for p, l in zip(pred_ids, tokenized[\"test\"][\"labels\"]):\n",
    "    pt, lt = [], []\n",
    "    for pi, li in zip(p, l):\n",
    "        if li == -100:\n",
    "            continue\n",
    "        pt.append(id2label[int(pi)])\n",
    "        lt.append(id2label[int(li)])\n",
    "    pred_tags.append(pt); true_tags.append(lt)\n",
    "\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(true_tags, pred_tags))\n",
    "print(f\"Inference Time (s): {inference_time:.4f}\")\n",
    "\n",
    "# save minimal metrics\n",
    "import json\n",
    "with open(OUT_DIR/\"metrics.json\",\"w\") as f:\n",
    "    json.dump({\"f1_dev\": float(f1_dev), \"f1_test\": float(f1_test), \"inference_time_s\": float(inference_time)}, f, indent=2)\n",
    "print(f\"\\nSaved metrics to {OUT_DIR}/metrics.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
