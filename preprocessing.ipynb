{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBLfVh_RKITa","executionInfo":{"status":"ok","timestamp":1759865542260,"user_tz":240,"elapsed":10676,"user":{"displayName":"Chenxin Gu","userId":"01034049942334915769"}},"outputId":"727e04b1-98da-4568-88fc-ba7f6d39d65a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","deps installed\n"]}],"source":["!pip -q install transformers datasets seqeval peft accelerate\n","import os, sys, json, random, pathlib\n","from pathlib import Path\n","print(\"deps installed\")"]},{"cell_type":"code","source":["# === 1. Connect Google Drive ===\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# === 2. clone E3C rep ===\n","!rm -rf E3C-Corpus\n","!git clone --depth 1 https://github.com/hltfbk/E3C-Corpus.git\n","\n","# === 3. Define path ===\n","import os\n","drive_path = \"/content/drive/MyDrive/small_data_NER_project/raw\"\n","os.makedirs(drive_path, exist_ok=True)\n","\n","# === 4. Copy to Drive ===\n","!cp -r E3C-Corpus/preprocessed_data/clinical_entities/layer1/English {drive_path}/\n","\n","# === 5. Check ===\n","!ls {drive_path}/English | head\n","print(\"Finished cloning data to Google Drive → MyDrive/small_data_NER_project/raw/English/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5qQzdVxKXLx","executionInfo":{"status":"ok","timestamp":1759874961349,"user_tz":240,"elapsed":30673,"user":{"displayName":"Chenxin Gu","userId":"01034049942334915769"}},"outputId":"fef11e69-74cf-41cc-f7fd-440401d7e58e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Cloning into 'E3C-Corpus'...\n","remote: Enumerating objects: 51427, done.\u001b[K\n","remote: Counting objects: 100% (51427/51427), done.\u001b[K\n","remote: Compressing objects: 100% (43656/43656), done.\u001b[K\n","remote: Total 51427 (delta 8353), reused 50577 (delta 7761), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (51427/51427), 180.38 MiB | 16.70 MiB/s, done.\n","Resolving deltas: 100% (8353/8353), done.\n","Updating files: 100% (51361/51361), done.\n","test.txt\n","train.txt\n","Finished cloning data to Google Drive → MyDrive/small_data_NER_project/raw/English/\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","import random\n","from collections import Counter\n","\n","BASE = Path(\"/content/drive/MyDrive/small_data_NER_project\")\n","RAW  = BASE/\"raw/English\"\n","CONLL= BASE/\"conll\"\n","CONLL.mkdir(parents=True, exist_ok=True)\n","\n","def read_conll(p):\n","    sents, sent = [], []\n","    with open(p, encoding=\"utf-8\") as f:\n","        for line in f:\n","            line=line.strip()\n","            if not line:\n","                if sent: sents.append(sent); sent=[]\n","                continue\n","            parts=line.split()\n","            tok, lab = parts[0], parts[-1]\n","            lab = \"O\" if lab == \"0\" else lab  # normalize '0' -> 'O'\n","            sent.append((tok, lab))\n","    if sent: sents.append(sent)\n","    return sents\n","\n","def write_conll(sents, path):\n","    with open(path, \"w\", encoding=\"utf-8\") as w:\n","        for sent in sents:\n","            for tok, lab in sent:\n","                w.write(f\"{tok}\\t{lab}\\n\")\n","            w.write(\"\\n\")\n","\n","def has_entity(sent):\n","    return any(lab != \"O\" for _, lab in sent)\n","\n","def stratified_dev_split(train_sents, dev_ratio=0.1, min_dev=200, seed=42):\n","    \"\"\"Ensure dev has entities: split train into new_train/dev with entity presence.\"\"\"\n","    random.seed(seed)\n","    pos = [s for s in train_sents if has_entity(s)]\n","    neg = [s for s in train_sents if not has_entity(s)]\n","    n_dev = max(min_dev, int(len(train_sents) * dev_ratio))\n","    random.shuffle(pos); random.shuffle(neg)\n","    # proportional selection\n","    n_pos = min(len(pos), max(1, int(n_dev * (len(pos) / max(1, len(train_sents))))))\n","    dev = pos[:n_pos] + neg[:n_dev - n_pos]\n","    new_train = pos[n_pos:] + neg[n_dev - n_pos:]\n","    random.shuffle(new_train); random.shuffle(dev)\n","    return new_train, dev\n","\n","def label_stats(conll_path):\n","    counts = Counter()\n","    with open(conll_path, encoding=\"utf-8\") as f:\n","        for line in f:\n","            line=line.strip()\n","            if not line:\n","                continue\n","            lab = line.split()[-1]\n","            lab = \"O\" if lab == \"0\" else lab\n","            counts[lab] += 1\n","    total = sum(counts.values())\n","    print(f\"=== Label counts in {conll_path.name} (total tokens={total}) ===\")\n","    for k,v in counts.most_common():\n","        pct = 0.0 if total==0 else v/total*100\n","        print(f\"{k:12s} {v:7d}  ({pct:5.2f}%)\")\n","    print()\n","\n","# --- Step 1: Load original E3C splits ---\n","train_sents = read_conll(RAW/\"train.txt\")\n","test_sents  = read_conll(RAW/\"test.txt\")\n","print(f\"Loaded {len(train_sents)} train sentences, {len(test_sents)} test sentences\")\n","\n","# --- Step 2: Split dev (stratified; ensure entities present in dev) ---\n","new_train, dev_sents = stratified_dev_split(train_sents, dev_ratio=0.1, min_dev=200, seed=42)\n","print(f\"Split -> new_train={len(new_train)}  dev={len(dev_sents)}  test={len(test_sents)}\")\n","\n","# --- Step 3: Save unified CoNLL ---\n","write_conll(new_train, CONLL/\"train.conll\")\n","write_conll(dev_sents, CONLL/\"dev.conll\")\n","write_conll(test_sents, CONLL/\"test.conll\")\n","print(\"Saved unified CoNLL files to\", CONLL)\n","\n","# --- Step 4: Label distributions (instead of head) ---\n","label_stats(CONLL/\"train.conll\")\n","label_stats(CONLL/\"dev.conll\")\n","label_stats(CONLL/\"test.conll\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlziU7GGLAlA","executionInfo":{"status":"ok","timestamp":1759874961474,"user_tz":240,"elapsed":121,"user":{"displayName":"Chenxin Gu","userId":"01034049942334915769"}},"outputId":"2ee19d46-05cc-4ac2-8e22-0eabea613705"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 669 train sentences, 851 test sentences\n","Split -> new_train=469  dev=200  test=851\n","Saved unified CoNLL files to /content/drive/MyDrive/small_data_NER_project/conll\n","=== Label counts in train.conll (total tokens=9112) ===\n","O               8524  (93.55%)\n","B-ety            303  ( 3.33%)\n","I-ety            285  ( 3.13%)\n","\n","=== Label counts in dev.conll (total tokens=3545) ===\n","O               3285  (92.67%)\n","B-ety            134  ( 3.78%)\n","I-ety            126  ( 3.55%)\n","\n","=== Label counts in test.conll (total tokens=16702) ===\n","O              15746  (94.28%)\n","B-ety            516  ( 3.09%)\n","I-ety            440  ( 2.63%)\n","\n"]}]},{"cell_type":"code","source":["# ===== N-way K-shot sampler for E3C (writes to Drive) =====\n","from pathlib import Path\n","from collections import Counter, defaultdict\n","import random\n","\n","BASE   = Path(\"/content/drive/MyDrive/small_data_NER_project\")\n","CONLL  = BASE/\"conll\"\n","TRAINF = CONLL/\"train.conll\"\n","DEVF   = CONLL/\"dev.conll\"\n","TESTF  = CONLL/\"test.conll\"\n","\n","def read_conll(path: Path):\n","    sents, sent = [], []\n","    with open(path, encoding=\"utf-8\") as f:\n","        for line in f:\n","            line=line.strip()\n","            if not line:\n","                if sent: sents.append(sent); sent=[]\n","                continue\n","            tok, lab = line.split()[:2]\n","            lab = \"O\" if lab == \"0\" else lab\n","            sent.append((tok, lab))\n","    if sent: sents.append(sent)\n","    return sents\n","\n","def write_conll(sents, path: Path):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    with open(path, \"w\", encoding=\"utf-8\") as w:\n","        for sent in sents:\n","            for tok, lab in sent:\n","                w.write(f\"{tok}\\t{lab}\\n\")\n","            w.write(\"\\n\")\n","\n","def sentence_types(sent):\n","    return {lab.split(\"-\",1)[1] for _, lab in sent if lab.startswith(\"B-\")}\n","\n","def sentence_mention_counts(sent):\n","    c = Counter()\n","    for _, lab in sent:\n","        if lab.startswith(\"B-\"):\n","            c[lab.split(\"-\",1)[1]] += 1\n","    return c\n","\n","def corpus_mention_totals(sents):\n","    tot = Counter()\n","    for s in sents: tot.update(sentence_mention_counts(s))\n","    return tot\n","\n","def stratified_dev_if_missing(train_sents, ratio=0.1, min_dev=200, seed=42):\n","    random.seed(seed)\n","    has_ent = [s for s in train_sents if any(l!=\"O\" for _,l in s)]\n","    no_ent  = [s for s in train_sents if not any(l!=\"O\" for _,l in s)]\n","    n_dev = max(min_dev, int(len(train_sents)*ratio))\n","    random.shuffle(has_ent); random.shuffle(no_ent)\n","    k_has = min(len(has_ent), max(1, int(n_dev * (len(has_ent)/(len(train_sents)+1e-9)))))\n","    dev = has_ent[:k_has] + no_ent[:n_dev-k_has]\n","    tr  = has_ent[k_has:] + no_ent[n_dev-k_has:]\n","    random.shuffle(tr); random.shuffle(dev)\n","    return tr, dev\n","\n","def build_fewshot(k=5, seed=42, mode=\"sent\"):\n","    \"\"\"\n","    mode='sent': ensure each entity type appears in >=K sentences (if possible)\n","    mode='mention': ensure each entity type has >=K mentions (if possible)\n","    \"\"\"\n","    assert mode in (\"sent\",\"mention\")\n","    train = read_conll(TRAINF)\n","    dev   = read_conll(DEVF) if DEVF.exists() else None\n","    test  = read_conll(TESTF)\n","    if dev is None:\n","        train, dev = stratified_dev_if_missing(train, ratio=0.1, min_dev=200, seed=seed)\n","\n","    types = sorted(corpus_mention_totals(train).keys())\n","    random.seed(seed)\n","\n","    if mode == \"sent\":\n","        # Greedy: pick sentence that increases #types still below K\n","        picked=set()\n","        covered = Counter()  # per-type sentence count\n","        def gain(i):\n","            g=0\n","            for t in sentence_types(train[i]):\n","                if covered[t] < k: g += 1\n","            return g\n","        # Upper bound per type cannot exceed available sentences containing that type\n","        contain = defaultdict(list)\n","        for i, s in enumerate(train):\n","            for t in sentence_types(s): contain[t].append(i)\n","        while True:\n","            if all(covered[t] >= min(k, len(contain[t])) for t in types):\n","                break\n","            best_i, best_g = -1, 0\n","            for i in range(len(train)):\n","                if i in picked: continue\n","                g = gain(i)\n","                if g > best_g:\n","                    best_g, best_i = g, i\n","            if best_i == -1 or best_g == 0: break\n","            picked.add(best_i)\n","            for t in sentence_types(train[best_i]):\n","                covered[t] += 1\n","\n","    else:  # mode == \"mention\"\n","        totals = corpus_mention_totals(train)\n","        target = {t: min(k, totals[t]) for t in types}\n","        per_sent = [sentence_mention_counts(s) for s in train]\n","        picked=set(); covered=Counter()\n","        def gain(i):\n","            g=0; cnt=per_sent[i]\n","            for t, need in target.items():\n","                if covered[t] >= need: continue\n","                if t in cnt:\n","                    g += min(cnt[t], need - covered[t])\n","            return g\n","        while True:\n","            if all(covered[t] >= target[t] for t in types): break\n","            best_i, best_g = -1, 0\n","            for i in range(len(train)):\n","                if i in picked: continue\n","                g = gain(i)\n","                if g > best_g: best_g, best_i = g, i\n","            if best_i == -1 or best_g == 0: break\n","            picked.add(best_i); covered.update(per_sent[best_i])\n","\n","    few_idxs = sorted(picked)\n","    few_train = [train[i] for i in few_idxs]\n","\n","    out_dir = CONLL/f\"fewshot_k{k}_seed{seed}_{mode}\"\n","    write_conll(few_train, out_dir/\"train.conll\")\n","    write_conll(dev,       out_dir/\"dev.conll\")\n","    write_conll(test,      out_dir/\"test.conll\")\n","\n","    # Print coverage stats\n","    print(f\"\\n== Built few-shot @ {out_dir} ==\")\n","    print(f\"train sentences = {len(few_train)}, dev={len(dev)}, test={len(test)}\")\n","    if mode == \"sent\":\n","        per_type_sent = Counter()\n","        for s in few_train:\n","            for t in sentence_types(s): per_type_sent[t]+=1\n","        print(\"Per-type sentence coverage (aim >= K):\")\n","        for t in types:\n","            print(f\"  {t:15s} {per_type_sent[t]:3d}\")\n","    else:\n","        c = corpus_mention_totals(few_train)\n","        print(\"Per-type mention coverage (aim >= K):\")\n","        for t in types:\n","            print(f\"  {t:15s} {c[t]:3d}\")\n","\n","    return out_dir\n","\n","# ---- Build NK sets you need (edit K_LIST if desired) ----\n","K_LIST = (1,5,10,20)\n","for K in K_LIST:\n","    build_fewshot(k=K, seed=42, mode=\"sent\")     # each type ≥ K sentences\n","    build_fewshot(k=K, seed=42, mode=\"mention\")  # each type ≥ K mentions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJ8wiYBIqzym","executionInfo":{"status":"ok","timestamp":1759874962467,"user_tz":240,"elapsed":827,"user":{"displayName":"Chenxin Gu","userId":"01034049942334915769"}},"outputId":"7b351225-4da0-445a-c1bd-573b9b9fb854"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k1_seed42_sent ==\n","train sentences = 1, dev=200, test=851\n","Per-type sentence coverage (aim >= K):\n","  ety               1\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k1_seed42_mention ==\n","train sentences = 1, dev=200, test=851\n","Per-type mention coverage (aim >= K):\n","  ety               1\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k5_seed42_sent ==\n","train sentences = 5, dev=200, test=851\n","Per-type sentence coverage (aim >= K):\n","  ety               5\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k5_seed42_mention ==\n","train sentences = 1, dev=200, test=851\n","Per-type mention coverage (aim >= K):\n","  ety               5\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k10_seed42_sent ==\n","train sentences = 10, dev=200, test=851\n","Per-type sentence coverage (aim >= K):\n","  ety              10\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k10_seed42_mention ==\n","train sentences = 2, dev=200, test=851\n","Per-type mention coverage (aim >= K):\n","  ety              10\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k20_seed42_sent ==\n","train sentences = 20, dev=200, test=851\n","Per-type sentence coverage (aim >= K):\n","  ety              20\n","\n","== Built few-shot @ /content/drive/MyDrive/small_data_NER_project/conll/fewshot_k20_seed42_mention ==\n","train sentences = 4, dev=200, test=851\n","Per-type mention coverage (aim >= K):\n","  ety              20\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","from pathlib import Path\n","\n","def count_cover_and_mentions(dirpath):\n","    sents = read_conll(Path(dirpath)/\"train.conll\")\n","    # 句子覆盖\n","    per_type_sent = Counter()\n","    for s in sents:\n","        for t in sentence_types(s):\n","            per_type_sent[t]+=1\n","    # 提及覆盖\n","    per_type_mentions = corpus_mention_totals(sents)\n","    print(\"Per-type sentence coverage:\", dict(per_type_sent))\n","    print(\"Per-type mention  coverage:\", dict(per_type_mentions))\n","\n","count_cover_and_mentions(CONLL/\"fewshot_k5_seed42_sent\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-yPgrGJrWEx","executionInfo":{"status":"ok","timestamp":1759874962483,"user_tz":240,"elapsed":12,"user":{"displayName":"Chenxin Gu","userId":"01034049942334915769"}},"outputId":"a96cccff-9b2f-4f4a-e4f8-fb4555aee5e7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Per-type sentence coverage: {'ety': 5}\n","Per-type mention  coverage: {'ety': 9}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cEs5StBFuIva"},"execution_count":null,"outputs":[]}]}